# -*- coding: utf-8 -*-
"""Project_2_(Prudhviraj_Sheela).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16_MAtybtbSVG91GY_VtqQk9hE1QS2maP
"""

# ENTER YOUR NAME AND ID HERE
# Name : Prudhviraj Sheela
# OSU CWID : A20228857

import random
import os
import numpy as np
from random import randrange
from shutil import copyfile
import math

#MY_ID = 'ENTER THE LAST 3 DIGITS OF YOUR ID'
# MY_ID should be string
# Example MY_ID='819'
MY_ID = '857'

task_dict = {0:'taska',1:'taskb',2:'taskc',3:'taskd',4:'taske'}
n_1 = 0
n_2 = 0
n_3 = 0
id_dict = {0:n_1,1:n_2,2:n_3}

try:
    n_2 = int(MY_ID[-2]) % 5
    n_3 = int(MY_ID[-3]) % 5
    n_1 = int(MY_ID[-1]) % 5
    
    while n_1 == n_2 or n_1 == n_3:
        n_1 = (n_1 + 1) % 5
    
    while n_1 == n_2 or n_2 == n_3:
        n_2 = (n_2 + 1) % 5


        
except Exception as e:
    print('Please enter a valid ID...')

data_path = '/content/corpus_data' 
# Edit this path if the data directory is not in the current directory

try:
    os.makedirs('Data_Sample')
    os.makedirs('Original_Sample')
except Exception as e:
    pass

for i in range(3):
    task = task_dict[id_dict[i]]
    
    for file in os.listdir(data_path):
        if task in file:
            if 'orig' in file:
                copyfile(data_path + '/' + file, 'Original_Sample/' + file)
            else:
                copyfile(data_path + '/' + file, 'Data_Sample/' + file)

# Type your code here... 
# Create necessary number of cells below this cell

#Step - 1a --> In this step we do the data preprocessing on all the text files and collect all the documents in the dictionary for finding the shingles in the next steps
import re
from pathlib import Path
d = {} # This stores each file with all its set of words for data_sample
directory = '/content/Data_Sample/' # Directory Path of Data Sample
data = '' # We are initializing the empty string to store individual data file
for file in os.listdir(directory): # Reading each file from the directory
  filepath = directory + file # Combining the directory with the text file which obtains the filepath
  if filepath.endswith(".txt"): # We check if the file path ends with ".txt"
    with open(filepath,'rb') as f: # Opening the file in read as bytes mode for avoiding encoding and decoding processes
      data = f.read() # Reading the file contents
      words = data.split() # Here we are splitting the data from each file and storing them as individual words
      final_doc = [] # This contains the list of words for each document
      for i in words:
        i = str(i)[2:-1] # Here we are stripping the bytes string to normal string
        final_doc.append(i.lower()) # Converting each word into lower matrix
      j = file # Assigning the filename corresponding
      d[j] = final_doc # This stores the words from each file
print(d)

# Step - 1b --> In this step we find the unique count of k-shingles for (3,4,5)
k = 3 # Initial Shingle Size
while True and k<=5: # Iterate for all the shingles until size '5'
  if k==3:
    shinglesInDocWords = set()
    count_3 = 0 # Count initailized for 3 shingles
    for i in d: # Iterate through all documents for finding the shingles
      words = d[i]
      shingle = []
      for index in range(len(words) - k + 1):
        shingle = words[index:index + k] # Here the unique shingles are obtained in the list
        shingle = ' '.join(shingle) # Combining them to words of length 3
        if shingle not in shinglesInDocWords: # Checking if the shingle exists and adding it to the document dictionary 
          shinglesInDocWords.add(shingle)
          count_3 = count_3 + 1 # Increment the count for each shingle obtained
    k += 1
  elif k==4:
    shinglesInDocWords = set()
    count_4 = 0 # Count initailized for 4 shingles
    for i in d: # Iterate through all documents for finding the shingles
      words = d[i]
      shingle = []
      for index in range(len(words) - k + 1):
        shingle = words[index:index + k] # Here the unique shingles are obtained in the list
        shingle = ' '.join(shingle) # Combining them to words of length 4
        if shingle not in shinglesInDocWords: # Checking if the shingle exists and adding it to the document dictionary 
          shinglesInDocWords.add(shingle)
          count_4 = count_4 + 1 # Increment the count for each shingle obtained
    k += 1
  else:
    shinglesInDocWords = set()
    count_5 = 0 # Count initailized for 5 shingles
    for i in d: # Iterate through all documents for finding the shingles
      words = d[i]
      shingle = []
      for index in range(len(words) - k + 1):
        shingle = words[index:index + k] # Here the unique shingles are obtained in the list
        shingle = ' '.join(shingle) # Combining them to words of length 5
        if shingle not in shinglesInDocWords: # Checking if the shingle exists and adding it to the document dictionary 
          shinglesInDocWords.add(shingle)
          count_5 = count_5 + 1 # Increment the count for each shingle obtained
    k+=1

print("Count of 3-Shingles is ",count_3) # Count of 3-Shingles 
print("Count of 4-Shingles is ",count_4) # Count of 4-Shingles 
print("Count of 5-Shingles is ",count_5) # Count of 5-Shingles

# Type your code to get the 5-shingle index here
docAsShingleSets = {} # This stores my document Shingle Sets
docNames = [] # This list stores the file names
k = 5 # For 5-shingle index
for i in d:
  words = d[i] # Now we start looping through each document to get the all 5-shingles related to it
  docid = i
  docNames.append(docid) # This stores the file names as list which will be used for future reference for obtaining signature matrices
  shinglesInDocWords = set()
  for index in range(len(words) - k + 1): 
    shingle = words[index:index + k] # Here we are obtaining all the 5-shingles for each documents words
    shingle = ' '.join(shingle) # Combining them to words of length 5
    if shingle not in shinglesInDocWords: # Checking if the shingle exists and adding it to the document dictionary 
      shinglesInDocWords.add(shingle) 
    else: # If there is any empty shingle it is deleted and index size is reduced
      del shingle 
      index = index - 1
  docAsShingleSets[docid] = shinglesInDocWords 

print(docAsShingleSets) # This shows the 5-shingles for each file
value_5 = [] # This stores all the 5-shingles from each file into one.
for key,value in docAsShingleSets.items():
  for item in value:
    value_5.append(item)
# print(value_5)

# ***************************************************NEW***********************************************************
# Generate Hash functions - 
    # We use (ax + b) mod N formula to permute shingle index
    # where a,b are random numbers, N total index size, and x is the index
# We need to do L permutations - In other words we need to have L permutations (lists) of new indexes
# Following function takes total index size N and L as arguments
    # And returns L new lists of size N
    
def get_hash_functions(N,L):
    hash_functions = []
    
    for itr in range(L):
        a=randrange(1,400)
        b=randrange(1,400)
        
        new_hash_function = []
        for i in range(N):
            new_hash_function.append((a * i + b) % N)
        
        hash_functions.append(new_hash_function)
    return hash_functions
        
# test
# hash_functions = get_hash_functions(5000,50)

# Type your code here to generate all L hash functions
# Generate hash functions only for shingle index created for k=5
N = len(value_5) # Given value in the question to consider 4-shingles as value of 'N'
L1 = 50
hash_functions_50 = get_hash_functions(N,L1) # Hash Functions of 50
#print(hash_functions_50)

L2 = 100
hash_functions_100 = get_hash_functions(N,L2) # Hash Functions of 100
#print(hash_functions_100)

L3 = 200
hash_functions_200 = get_hash_functions(N,L3) # Hash Functions of 200
#print(hash_functions_200)

L4 = 500
hash_functions_500 = get_hash_functions(N,L4) # Hash Functions of 500
#print(hash_functions_500)

L5 = 1000
hash_functions_1000 = get_hash_functions(N,L5) # Hash Functions of 1000
#print(hash_functions_1000)

signatures_50 = [] # This is for signature matrix of 50
count2=0
for j in docAsShingleSets: 
  list_shingle = [math.inf for i in range(50)] # Initially we store all signature values to infinity
  count1 = 0
  for i in value_5: # Here we hold the each shingle
    if i in docAsShingleSets[j]: # Here we check if the shingle is present in the selected file shingles
      for count2 in range(50): # This iterates through the 50 hash functions
        if  hash_functions_50[count2][count1] < list_shingle[count2]: # This compares if the hash_function_50 is less than list_shingle or not
          list_shingle[count2] = hash_functions_50[count2][count1]
    count1 = count1 + 1 # We increment through each document
  signatures_50.append(list_shingle)
mat_50 = np.array(signatures_50) # Converting signatures_50 into array
mat_50 = mat_50.T # Obtaining the transpose of the form (50,19)
mat_50 = np.mat(mat_50) # Converting the array into matrices and we find mat_50 (Stores the signature matrix of 50)
# mat_50

signatures_100 = [] # This is for signature matrix of 100
count2=0
for j in docAsShingleSets:
  list_shingle = [math.inf for i in range(100)] # Initially we store all signature values to infinity
  count1 = 0
  for i in value_5: # Here we hold the each shingle
    if i in docAsShingleSets[j]: # Here we check if the shingle is present in the selected file shingles
      for count2 in range(100):  # This iterates through the 100 hash functions
        if  hash_functions_100[count2][count1] < list_shingle[count2]: # This compares if the hash_function_100 is less than list_shingle or not
          list_shingle[count2] = hash_functions_100[count2][count1]
    count1 = count1 + 1 # We increment through each document
  signatures_100.append(list_shingle)
mat_100 = np.array(signatures_100) # Converting signatures_100 into array
mat_100 = mat_100.T # Obtaining the transpose of the form (100,19)
mat_100 = np.mat(mat_100) # Converting the array into matrices and we find mat_100 (Stores the signature matrix of 100)
# mat_100

signatures_200 = [] # This is for signature matrix of 200
count2=0
for j in docAsShingleSets:
  list_shingle = [math.inf for i in range(200)] # Initially we store all signature values to infinity
  count1 = 0
  for i in value_5: # Here we hold the each shingle
    if i in docAsShingleSets[j]: # Here we check if the shingle is present in the selected file shingles
      for count2 in range(200):  # This iterates through the 200 hash functions
        if  hash_functions_200[count2][count1] < list_shingle[count2]: # This compares if the hash_function_200 is less than list_shingle or not
          list_shingle[count2] = hash_functions_200[count2][count1]
    count1 = count1 + 1 # We increment through each document
  signatures_200.append(list_shingle)
mat_200 = np.array(signatures_200)  # Converting signatures_200 into array
mat_200 = mat_200.T # Obtaining the transpose of the form (200,19)
mat_200 = np.mat(mat_200) # Converting the array into matrices and we find mat_200 (Stores the signature matrix of 200)
# mat_200

signatures_500 = [] # This is for signature matrix of 500
count2=0
for j in docAsShingleSets:
  list_shingle = [math.inf for i in range(500)] # Initially we store all signature values to infinity
  count1 = 0
  for i in value_5: # Here we hold the each shingle
    if i in docAsShingleSets[j]: # Here we check if the shingle is present in the selected file shingles
      for count2 in range(500): # This iterates through the 500 hash functions
        if  hash_functions_500[count2][count1] < list_shingle[count2]: # This compares if the hash_function_500 is less than list_shingle or not
          list_shingle[count2] = hash_functions_500[count2][count1]
    count1 = count1 + 1  # We increment through each document
  signatures_500.append(list_shingle)
mat_500 = np.array(signatures_500) # Converting signatures_500 into array
mat_500 = mat_500.T # Obtaining the transpose of the form (500,19)
mat_500 = np.mat(mat_500) # Converting the array into matrices and we find mat_500 (Stores the signature matrix of 500)
# mat_500

signatures_1000 = [] # This is for signature matrix of 1000
count2=0
for j in docAsShingleSets:
  list_shingle = [math.inf for i in range(1000)] # Initially we store all signature values to infinity
  count1 = 0
  for i in value_5: # Here we hold the each shingle
    if i in docAsShingleSets[j]: # Here we check if the shingle is present in the selected file shingles
      for count2 in range(1000): # This iterates through the 1000 hash functions
        if  hash_functions_1000[count2][count1] < list_shingle[count2]: # This compares if the hash_function_1000 is less than list_shingle or not
          list_shingle[count2] = hash_functions_1000[count2][count1]
    count1 = count1 + 1 # We increment through each document
  signatures_1000.append(list_shingle)
mat_1000 = np.array(signatures_1000) # Converting signatures_1000 into array
mat_1000 = mat_1000.T # Obtaining the transpose of the form (1000,19)
mat_1000 = np.mat(mat_1000) # Converting the array into matrices and we find mat_1000 (Stores the signature matrix of 1000)
#mat_1000

# Type your code here to do the fact check 
#      with any one query document in the 'Original_Sample' directory

original = '/content/Original_Sample/orig_taska.txt'

# STEP-1: Generate 5-shingles 
    # (if any shingles are not present in your shingle index, simply ignore them)
d_original = {} # This stores each file with all its set of words for original
data = ''  # We are initializing the empty string to store individual data file
with open(original,'rb') as f: # Opening the file in read as bytes mode for avoiding encoding and decoding processes
  data = f.read() # Reading the file contents
  words = data.split()  # Here we are splitting the data from each file and storing them as individual words
  final_doc = [] # This contains the list of words for original document
  for i in words:
    i = str(i)[2:-1] # Here we are stripping the bytes string to normal string
    final_doc.append(i.lower()) # Converting each word into lower matrix
    j = original # Assigning the filename corresponding
    d_original[j] = final_doc  # This stores the words from each file
#print(d_original)
#print(len(d_original))

docAsShingleSets1 = {} # This stores my document Shingle Sets
k = 5 # For 5-shingle index
words = d_original[original] # Now we assign document to get the all 5-shingles related to it
shinglesInDocWords1 = [] # This stores the 5-shingles of that particular document
for index in range(len(words) - k + 1):
  shingle = words[index:index + k] # Here we are obtaining all the 5-shingles for each documents words
  shingle = ' '.join(shingle) # Combining them to words of length-5
  if shingle not in shinglesInDocWords1: # Checking if the shingle exists and adding it to the document's list
    shinglesInDocWords1.append(shingle)
#print(shinglesInDocWords1) # --> This stores all the shingles for original document file

# STEP-2: Generate signature vector from L hash functions

# Below we are generating signature vectors for 50 hash functions based on original document
count2=0
signatures_50_1 = [] # Here we store the signature list
list_shingle = [math.inf for i in range(50)] # Initializing the initial signature vectors to infinity
count1 = 0
for i in value_5: # In this below loop we will compare the hash function obtained for L and initalized shingles to get the signature vector
  if i in shinglesInDocWords1:
    for count2 in range(50):
      if  hash_functions_50[count2][count1] < list_shingle[count2]: # If this condition satisfies we update each element into the signature vector
        list_shingle[count2] = hash_functions_50[count2][count1] 
  count1 = count1 + 1
signatures_50_1.append(list_shingle)
matv_50 = np.array(signatures_50_1) # 
matv_50 = matv_50.T
matv_50 = np.mat(matv_50) # In this step we obtain the corresponding L's signature vector
#matv_50

# Below we are generating signature vectors for 100 hash functions based on original document
count2=0
signatures_100_1 = [] # Here we store the signature list
list_shingle = [math.inf for i in range(100)] # Initializing the initial signature vectors to infinity
count1 = 0
for i in value_5: # In this below loop we will compare the hash function obtained for L and initalized shingles to get the signature vector
  if i in shinglesInDocWords1:
    for count2 in range(100):
      if  hash_functions_100[count2][count1] < list_shingle[count2]: # If this condition satisfies we update each element into the signature vector
        list_shingle[count2] = hash_functions_100[count2][count1]
  count1 = count1 + 1
signatures_100_1.append(list_shingle)
matv_100 = np.array(signatures_100_1)
matv_100 = matv_100.T
matv_100 = np.mat(matv_100) # In this step we obtain the corresponding L's signature vector
#matv_100

# Below we are generating signature vectors for 200 hash functions based on original document
count2=0
signatures_200_1 = [] # Here we store the signature list
list_shingle = [math.inf for i in range(200)] # Initializing the initial signature vectors to infinity
count1 = 0
for i in value_5: # In this below loop we will compare the hash function obtained for L and initalized shingles to get the signature vector
  if i in shinglesInDocWords1:
    for count2 in range(200):
      if  hash_functions_200[count2][count1] < list_shingle[count2]: # If this condition satisfies we update each element into the signature vector
        list_shingle[count2] = hash_functions_200[count2][count1]
  count1 = count1 + 1
signatures_200_1.append(list_shingle)
matv_200 = np.array(signatures_200_1)
matv_200 = matv_200.T
matv_200 = np.mat(matv_200) # In this step we obtain the corresponding L's signature vector
#matv_200

# Below we are generating signature vectors for 500 hash functions based on original document
count2=0
signatures_500_1 = [] # Here we store the signature list
list_shingle = [math.inf for i in range(500)] # Initializing the initial signature vectors to infinity
count1 = 0
for i in value_5: # In this below loop we will compare the hash function obtained for L and initalized shingles to get the signature vector
  if i in shinglesInDocWords1:
    for count2 in range(500):
      if  hash_functions_500[count2][count1] < list_shingle[count2]: # If this condition satisfies we update each element into the signature vector
        list_shingle[count2] = hash_functions_500[count2][count1]
  count1 = count1 + 1
signatures_500_1.append(list_shingle)
matv_500 = np.array(signatures_500_1)
matv_500 = matv_500.T
matv_500 = np.mat(matv_500) # In this step we obtain the corresponding L's signature vector
#matv_500

# Below we are generating signature vectors for 1000 hash functions based on original document
count2=0
signatures_1000_1 = []  # Here we store the signature list
list_shingle = [math.inf for i in range(1000)] # Initializing the initial signature vectors to infinity
count1 = 0
for i in value_5: # In this below loop we will compare the hash function obtained for L and initalized shingles to get the signature vector
  if i in shinglesInDocWords1:
    for count2 in range(1000):
      if  hash_functions_1000[count2][count1] < list_shingle[count2]: # If this condition satisfies we update each element into the signature vector
        list_shingle[count2] = hash_functions_1000[count2][count1]
  count1 = count1 + 1
signatures_1000_1.append(list_shingle)
matv_1000 = np.array(signatures_1000_1)
matv_1000 = matv_1000.T
matv_1000 = np.mat(matv_1000) # In this step we obtain the corresponding L's signature vector
#matv_1000

"""***NEW*** For each L = {50,100,200,500,1000}, report all documents (file_names) below that have Jaccard similarity > t=0.85 Sort the documents in decreasing order of the Jaccard similarity

"""

# STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.
    # and all other documents 

t = 0.2

# Step-3a: Calculating Jaccard Similarity for Signature Vector of Original Document

# Calculating Jacard Similarity for 50 hash functions
lista = [] # The list is used to store Jaccard Similarity values obtained
for i in range(19): # Range of Documents
  counter = 0 # Keeps Track of Occurences
  for j in range(50): # Range of Hash Functions
    if mat_50[j,i] == matv_50[j,0]: # We check if the signature matrix value is similar to the vector and increment the counter
      counter = counter + 1
  jcd = (counter / 50)
  lista.append(jcd)
file_names = docNames # Here we store the filenames
jcd_50 = {} # Stores the Jaccard Similarities for that particular document
for a,i in zip(lista,file_names): # In the below loop we attach each jaccard similarity obtained with its associated document and check if its threshold satisfies.
  if a > t:
    jcd_50[i] = a
jcd_50 = sorted(jcd_50.items(),key = lambda x : x[1], reverse = True) # Sort Jaccard Similarities in Decreasing Order
print(jcd_50)

# Calculating Jacard Similarity for 100 hash functions
lista = [] # The list is used to store Jaccard Similarity values obtained
for i in range(19):  # Range of Documents
  counter = 0 # Keeps Track of Occurences
  for j in range(100): # Range of Hash Functions
    if mat_100[j,i] == matv_100[j,0]:  # We check if the signature matrix value is similar to the vector and increment the counter
      counter = counter + 1
  jcd = (counter / 100)
  lista.append(jcd)
file_names = docNames # Here we store the filenames
jcd_100 = {} # Stores the Jaccard Similarities for that particular document
for a,i in zip(lista,file_names): # In the below loop we attach each jaccard similarity obtained with its associated document and check if its threshold satisfies.
  if a > t:
    jcd_100[i] = a
jcd_100 = sorted(jcd_100.items(),key = lambda x : x[1], reverse = True) # Sort Jaccard Similarities in Decreasing Order
print(jcd_100)

# Calculating Jacard Similarity for 200 hash functions
lista = [] # The list is used to store Jaccard Similarity values obtained
for i in range(19): # Range of Documents
  counter = 0 # Keeps Track of Occurences
  for j in range(200):  # Range of Hash Functions
    if mat_200[j,i] == matv_200[j,0]:  # We check if the signature matrix value is similar to the vector and increment the counter
      counter = counter + 1
  jcd = (counter / 200)
  lista.append(jcd)
file_names = []
file_names = docNames # Here we store the filenames
jcd_200 = {}  # Stores the Jaccard Similarities for that particular document
for a,i in zip(lista,file_names): # In the below loop we attach each jaccard similarity obtained with its associated document and check if its threshold satisfies.
  if a > t:
    jcd_200[i] = a
jcd_200 = sorted(jcd_200.items(),key = lambda x : x[1], reverse = True) # Sort Jaccard Similarities in Decreasing Order
print(jcd_200)

# Calculating Jacard Similarity for 500 hash functions
lista = [] # The list is used to store Jaccard Similarity values obtained
for i in range(19): # Range of Documents
  counter = 0 # Keeps Track of Occurences
  for j in range(500): # Range of Hash Functions
    if mat_500[j,i] == matv_500[j,0]: # We check if the signature matrix value is similar to the vector and increment the counter
      counter = counter + 1 
  jcd = (counter / 500)
  lista.append(jcd)
file_names = docNames # Here we store the filenames
jcd_500 = {}  # Stores the Jaccard Similarities for that particular document
for a,i in zip(lista,file_names): # In the below loop we attach each jaccard similarity obtained with its associated document and check if its threshold satisfies.
  if a > t:
    jcd_500[i] = a
jcd_500 = sorted(jcd_500.items(),key = lambda x : x[1], reverse = True) # Sort Jaccard Similarities in Decreasing Order
print(jcd_500)

# Calculating Jacard Similarity for 1000 hash functions
lista = [] # The list is used to store Jaccard Similarity values obtained
for i in range(19): # Range of Documents
  counter = 0 # Keeps Track of Occurences
  for j in range(1000): # Range of Hash Functions
    if mat_1000[j,i] == matv_1000[j,0]: # We check if the signature matrix value is similar to the vector and increment the counter
      counter = counter + 1
  jcd = (counter / 1000)
  lista.append(jcd)
file_names = docNames  # Here we store the filenames
jcd_1000 = {} # Stores the Jaccard Similarities for that particular document
for a,i in zip(lista,file_names): # In the below loop we attach each jaccard similarity obtained with its associated document and check if its threshold satisfies.
  if a > t:
    jcd_1000[i] = a
jcd_1000 = sorted(jcd_1000.items(),key = lambda x : x[1], reverse = True)  # Sort Jaccard Similarities in Decreasing Order
print(jcd_1000)

"""**STEP - 3: LSH (30 points)**"""

# Type your code here to hash signature matrix into B buckets
# Use the technique to split the signature matrix into b bands of r rows
# Convert only the signature matrix generated with L=1000

b = 50
r = 20
B = 199
mat_1000 = np.array(mat_1000) # Convertin our signature matrix into array in order to reshape it

mat_1000_3d= mat_1000.reshape((b,r,len(docNames))) # Reshaping the array into a 3-D matrix which has b bands and r rows with the number of documents length

import collections
from collections import defaultdict

a=random.sample(range(100), r) # Randomly generating a sample of 'a' values that are to be inserted into the hash signature matrix
#print(a)

bucket = {} # We get all the documents obtained into this bucket after applying the below function
for i in range(b): # Range of bands
  dict1 = collections.defaultdict(list) # Used to store the list of documents corresponsing to a bucket
  for j in range(len(docNames)): # Range of documents
    h = (np.sum((a)*(mat_1000_3d[i,:,j])))%B # Applied Hash Signature
    dict1[h].append(docNames[j]) # Updating the bucket with its document names
  bucket.update(dict1)
main_bucket= collections.defaultdict(set) # This bucket stores the candidate items
for i in bucket: 
  if len(bucket[i]) > 1: # We consider buckets which have more than one document in candidate items
    for item in bucket[i]:
      main_bucket[i].add(item) # We add it into the main_buckets dictionary
print(main_bucket)

# Type your code here to do generate candidate documents
# Follow all steps from STEP - 2 fact check (except the Jaccard similarity part)

# STEP - 1: Split your original document signature vector into b bands of r rows

matv_1000 = np.array(matv_1000) # Converting the signature vector to array for reshaping it
matv_1000_3d= matv_1000.reshape((b,r,1)) # Respahing the array

# STEP - 2: Hash using the same hash functions created for 
    # signature matrix hashing (in the previous cell)

dict2 = collections.defaultdict(list) # Used to store the list of documents corresponsing to a bucket
for i in range(50):  # Range of bands
  for j in range(1):  # Range of document
    h = (np.sum((a)*(matv_1000_3d[i,:,j])))%B # Applied Hash Signature
    if main_bucket[h] == set(): # If there exists an empty set we just skip it from the bucket
      continue
    else:
      dict2[h].append(main_bucket[h]) # Updating the bucket with its document names
print(dict2)
candidate_items = set() # For storing all the obtained documents into one set
for i in dict2: # Iterating through each candidate dictionary
  for j in dict2[i]: # Iterating through each key for getting its value
    for item in j:
      candidate_items.add(item) # Storing all the documents into candidate items
candidate_items = list(candidate_items)
print(candidate_items)

candidate_items_dict = {} # Initializing the candidate items dictionary
for i in candidate_items: # We try to again obtain the corresponding candidate_items text into a document for finding the jaccard similarities
  for j in docAsShingleSets:
    if i == j:
      candidate_items_dict[i] = docAsShingleSets[j]
print(candidate_items_dict) # This will form all the candidate_items with their content stored in it
value_cand_items = [] # This stores all the 5-shingles from each candidate_item into one list
for key,value in candidate_items_dict.items(): # Below is the loop running for obtaining them
  for item in value:
    value_cand_items.append(item)
#print(value_cand_items)

sig_mat_candidate = [] # This is for signature matrix of candidate items
count2=0
for j in candidate_items_dict: # Iterate through every candidate dictionary
  list_shingle = [math.inf for i in range(1000)] # Initially we store all signature values to infinity
  count1 = 0
  for i in value_cand_items: # Here we hold the each shingle
    if i in candidate_items_dict[j]: # Here we check if the shingle is present in the selected file shingles
      for count2 in range(1000): # This iterates through the 1000 hash functions
        if  hash_functions_1000[count2][count1] < list_shingle[count2]: # This compares if the hash_function_1000 is less than list_shingle or not
          list_shingle[count2] = hash_functions_1000[count2][count1]
    count1 = count1 + 1 # We increment through each document
  sig_mat_candidate.append(list_shingle)
matc_1000 = np.array(sig_mat_candidate) # Converting signatures_1000 into array
matc_1000 = matc_1000.T # Obtaining the transpose of the form (1000,3)
matc_1000 = np.mat(matc_1000) # Converting the array into matrices and we find mat_1000 (Stores the signature matrix of 1000)
matc_1000

"""**Report** all documents (file_names) below that have Jaccard similarity > t=0.85
Sort the documents in decreasing order of the Jaccard similarity

"""

documents_similar = {}
# Calculating Jacard Similarity for 1000 hash functions
listc_jcd = [] # The list is used to store Jaccard Similarity values obtained
for i in range(3): # Range of Documents
  counter = 0 # Keeps Track of Occurences
  for j in range(1000): # Range of Hash Functions
    if matc_1000[j,i] == matv_1000[j,0]: # We check if the signature matrix value is similar to the vector and increment the counter
      counter = counter + 1
  jcd = (counter / 1000)
  listc_jcd.append(jcd)
print(listc_jcd)
file_names = candidate_items  # Here we store the candidate_items in filenames
documents_similar_sorted = {} # Stores the Jaccard Similarities for that particular document
for a,i in zip(listc_jcd,file_names): # In the below loop we attach each jaccard similarity obtained with its associated document and check if its threshold satisfies.
  if a > t: 
    documents_similar_sorted[i] = a
#print(documents_similar_sorted)
documents_similar_sorted_value = sorted(documents_similar_sorted.items(),key = lambda x : x[1], reverse = True)  # Sort Jaccard Similarities in Decreasing Order
documents_similar_sorted_value = dict(documents_similar_sorted_value)
#print(documents_similar_sorted_value)

"""Report the list of false positives and false negatives below"""

# Reports the candidate documents that have jaccard similarity <=t
keys1 = candidate_items_dict.keys() # Considers to check if the candidate documents keys are 
keys2 = documents_similar_sorted_value.keys() # Documents that are similar in sorted fashion are obtained
false_pos = keys1 - keys2 # Stores the set of all 
false_pos_lst = list(false_pos)
false_pos_lst
print("List of all False Positives are : ",false_pos_lst)
print("Count of False Negatives = ",len(false_pos_lst))

# Reports the documents whose Jaccard similarity is >t and not in the candidate set
jcd_1000_dict = dict(jcd_1000) # Elements not in candidate set are considered as keys1
keys1 = jcd_1000_dict.keys()
keys2 = documents_similar_sorted_value.keys() # Documents that are similar in sorted fashion are obtained
false_neg = keys1 - keys2 # Stores the set of all documents that satisfy the jaccard similarity value
false_neg_lst = list(false_neg)
print("List of all False Negatives are : ",false_neg_lst)
print("Count of False Negatives = ",len(false_neg_lst))